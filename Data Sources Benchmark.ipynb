{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cf4634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.ml_train import train_models\n",
    "from utils.performance import calculate_metrics\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "year = datetime.now().date().year\n",
    "month = datetime.now().date().month\n",
    "day = datetime.now().date().day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d0ee0",
   "metadata": {},
   "source": [
    "### Select feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bcf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Either \"DRFP\" or \"QM\"\n",
    "features = \"DRFP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03f5d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/HTE_BH_10_iter_9-3_Clusters_v2025-11.pkl\", 'rb') as f:\n",
    "    df_HTE_BH = pickle.load(f)\n",
    "\n",
    "# Initialize an empty DataFrame to hold the performance results\n",
    "df_perf = pd.DataFrame(columns=['Model', 'train_data', 'test_data', \n",
    "                                'Balanced Accuracy', 'ROC AUC', 'F1 Score', 'F0 Score', \n",
    "                                'AU-PR-C', 'Precision 1', 'Recall 1',\n",
    "                                'Balanced Accuracy Avg', 'Balanced Accuracy Std',\n",
    "                                'ROC AUC Avg', 'ROC AUC Std',\n",
    "                                'F1 Score Avg', 'F1 Score Std', \n",
    "                                'F0 Score Avg', 'F0 Score Std', \n",
    "                                'AU-PR-C Avg', 'AU-PR-C Std',\n",
    "                                'Precision 1 Avg', 'Precision 1 Std',\n",
    "                                'Recall 1 Avg', 'Recall 1 Std'])\n",
    "\n",
    "# Data for models\n",
    "train_data_source = []\n",
    "test_data_source = [] \n",
    "\n",
    "model_names = [\"rf_model\", \"gradient_boosting_model\", \"lr_model\", \"mlp_model\", \"knn_model\", \"gaussianNB_model\"]\n",
    "model_source = []\n",
    "\n",
    "\n",
    "train_sources = list(df_HTE_BH.Source.unique())\n",
    "\n",
    "# Prepare structure for DataFrame\n",
    "for main_source in train_sources:\n",
    "    for model in model_names:\n",
    "        model_source.append(model)\n",
    "        train_data_source.append(main_source) \n",
    "        test_data_source.append(\"Outter Clusters\")    \n",
    "\n",
    "df_perf['Model'] = model_source\n",
    "df_perf['train_data'] = train_data_source\n",
    "df_perf['test_data'] = test_data_source\n",
    "\n",
    "# Ensure that metrics columns are initialized with empty lists for all rows\n",
    "metrics_columns = ['Balanced Accuracy', 'ROC AUC', 'F1 Score', 'F0 Score', \n",
    "                   'AU-PR-C', 'Precision 1', 'Recall 1']\n",
    "\n",
    "for metric in metrics_columns:\n",
    "    df_perf[metric] = [[] for _ in range(len(df_perf))]\n",
    "\n",
    "# Run data preparation and training with different random states\n",
    "for run in tqdm(range(0, 10), desc=\"Total Runs\"):\n",
    "    \n",
    "    print(f\"Starting Run {run + 1}\")\n",
    "    \n",
    "    # Prepare the data for each run with a different random state\n",
    "    random_state_key = 0 + run  # Use different random states\n",
    "    \n",
    "    # Now loop through each main_source and model\n",
    "    for main_source, test_source in zip(train_data_source[0::len(model_names)], test_data_source[0::len(model_names)]):\n",
    "        \n",
    "        print(f\"Training on {main_source} and testing on {test_source} for run {run + 1}\")\n",
    "\n",
    "        # Prepare training data for main_source\n",
    "        df_train = df_HTE_BH[df_HTE_BH[\"Source\"] == main_source]\n",
    "\n",
    "        kcn = df_train[f\"iteration_{run} cluster\"].value_counts()\n",
    "        df_eval = df_HTE_BH[~df_HTE_BH[f\"iteration_{run} cluster\"].isin(list(kcn[kcn > 30].index))] \n",
    "\n",
    "        #ensure aryls and amines in train are not in test\n",
    "        df_eval = df_eval[~df_eval['Aryl SMILES'].isin(list(df_train['Aryl SMILES'].unique()))]\n",
    "        df_eval = df_eval[~df_eval['Amine SMILES'].isin(list(df_train['Amine SMILES'].unique()))]\n",
    "\n",
    "        # Shuffle the training data for each run\n",
    "        df_train = df_train.sample(frac=1, random_state=random_state_key)\n",
    "\n",
    "        # Train models\n",
    "        models, scaler = train_models(df_train, model_names, feats_col = features)\n",
    "        metrics, conf_bucket_metrics = calculate_metrics(models, scaler, df_eval, feats_col = features)\n",
    "\n",
    "        # Store each run's metrics\n",
    "        for model in models.keys():\n",
    "            for metric in metrics[model].keys():\n",
    "                # Append the result of this run to the corresponding list\n",
    "                df_perf.loc[(df_perf[\"Model\"] == model) & \n",
    "                            (df_perf[\"train_data\"] == main_source) & \n",
    "                            (df_perf[\"test_data\"] == test_source), metric].apply(lambda x: x.append(metrics[model][metric]))\n",
    "\n",
    "    # After all runs, calculate averages and standard deviations\n",
    "    for model in model_names:\n",
    "        for main_source, test_source in zip(train_data_source[0::len(model_names)], test_data_source[0::len(model_names)]):\n",
    "\n",
    "            for metric in metrics_columns:\n",
    "                metric_values = df_perf.loc[(df_perf[\"Model\"] == model) & \n",
    "                                            (df_perf[\"train_data\"] == main_source) & \n",
    "                                            (df_perf[\"test_data\"] == test_source), metric].values[0]\n",
    "                if metric_values is not None and isinstance(metric_values, list):\n",
    "                    avg = np.mean(metric_values)\n",
    "                    std = np.std(metric_values)\n",
    "                    df_perf.loc[(df_perf[\"Model\"] == model) & \n",
    "                                (df_perf[\"train_data\"] == main_source) & \n",
    "                                (df_perf[\"test_data\"] == test_source), f\"{metric} Avg\"] = avg\n",
    "                    df_perf.loc[(df_perf[\"Model\"] == model) & \n",
    "                                (df_perf[\"train_data\"] == main_source) & \n",
    "                                (df_perf[\"test_data\"] == test_source), f\"{metric} Std\"] = std\n",
    "                \n",
    "sel_metrics = ['ROC AUC Avg', 'F1 Score Avg']\n",
    "\n",
    "best_performance = df_perf.groupby('train_data').apply(\n",
    "    lambda x: x.loc[x[sel_metrics].sum(axis=1).idxmax()])\n",
    "\n",
    "best_performance.reset_index(drop=True, inplace=True)\n",
    "\n",
    "best_performance = best_performance[['Model', 'train_data', 'test_data', 'Balanced Accuracy Avg', 'Balanced Accuracy Std', 'ROC AUC Avg',\n",
    "                                     'ROC AUC Std', 'F1 Score Avg', 'F1 Score Std', 'F0 Score Avg',\n",
    "                                     'F0 Score Std', 'AU-PR-C Avg', 'AU-PR-C Std', 'Precision 1 Avg',\n",
    "                                     'Precision 1 Std', 'Recall 1 Avg', 'Recall 1 Std']]\n",
    "\n",
    "df_perf.to_csv(f\"results/Data_Source_{features}_all_{year}-{month}-{day}.csv\")\n",
    "best_performance.to_csv(f\"results/Data_Source_{features}_best_{year}-{month}-{day}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852c94f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4395cc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e1278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-bee_exp]",
   "language": "python",
   "name": "conda-env-.conda-bee_exp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
